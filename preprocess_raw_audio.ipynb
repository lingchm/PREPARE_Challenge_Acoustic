{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesss raw audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to perform the following preprocessing steps on raw audio files:\n",
    "\n",
    "1. Volume normalization \n",
    "2. Using deepfilternet for noise removal: https://github.com/Rikorose/DeepFilterNet\n",
    "3. Resampling of frequencies to 16 kHz since the high frequencies in the audio recordings typically do not contain speech\n",
    "5. Trim beginning and end silence \n",
    "6. Using spakder diarization for removal interviewer speech: https://huggingface.co/pyannote/speaker-diarization-3.1. Requires access to pyannote/segmentation-3.0 model as well.\n",
    "\n",
    "To use these models, need to login into huggingface and request gated acccess in your account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
    "# !pip install deepfilternet\n",
    "# !pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensmile\n",
    "import audiofile\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import os \n",
    "import soundfile as sf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from df.utils import download_file\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "from pyannote.core import Segment, Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_pause_time(y, sr, silence_threshold=0.02, frame_length=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Compute the total pause time (silent regions) in an audio file.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        silence_threshold (float): Energy threshold below which the region is considered a pause (silence).\n",
    "        frame_length (int): Length of each frame to analyze in samples.\n",
    "        hop_length (int): The number of samples to shift for each frame.\n",
    "    \n",
    "    Returns:\n",
    "        float: Total pause time in seconds.\n",
    "    \"\"\"\n",
    "    # Compute the energy (Root Mean Square) of the audio signal\n",
    "    energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    \n",
    "    # Identify silent regions where energy is below the threshold\n",
    "    silence = energy < silence_threshold\n",
    "    \n",
    "    # Get the time intervals of the silence\n",
    "    times = librosa.frames_to_time(np.arange(len(energy)), sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Calculate the total pause time\n",
    "    pause_time = 0.0\n",
    "    start = None\n",
    "    for i in range(len(silence)):\n",
    "        if silence[i] and start is None:\n",
    "            # Start of a silent region\n",
    "            start = times[i]\n",
    "        elif not silence[i] and start is not None:\n",
    "            # End of a silent region\n",
    "            end = times[i]\n",
    "            pause_time += (end - start)\n",
    "            start = None\n",
    "    \n",
    "    # If the audio ends in silence, add the final pause duration\n",
    "    if start is not None:\n",
    "        pause_time += (times[-1] - start)\n",
    "    \n",
    "    total_signal_time = librosa.get_duration(y=y, sr=sr)\n",
    "    \n",
    "    # Compute the ratio of pause time to signal time\n",
    "    ratio = pause_time / total_signal_time\n",
    "    \n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_silence(y, sr, silence_threshold=0.02, frame_length=2048, hop_length=512, output_path=None):\n",
    "    \"\"\"\n",
    "    Strip silence from an audio file and save the resulting audio without silence.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path (str): Path to the input audio file.\n",
    "        silence_threshold (float): Energy threshold below which the region is considered silent.\n",
    "        frame_length (int): Length of each frame to analyze in samples.\n",
    "        hop_length (int): The number of samples to shift for each frame.\n",
    "        output_path (str, optional): Path to save the output audio without silence. If None, it will return the numpy array.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Audio signal with silence removed (if no output_path is provided).\n",
    "    \"\"\"\n",
    "    # Compute the energy (Root Mean Square) of the audio signal\n",
    "    energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    \n",
    "    # Identify non-silent regions where energy is above the threshold\n",
    "    non_silent_frames = energy > silence_threshold\n",
    "    \n",
    "    # Get the time intervals corresponding to non-silent frames\n",
    "    times = librosa.frames_to_time(np.arange(len(energy)), sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Extract the non-silent portions of the audio\n",
    "    non_silent_audio = []\n",
    "    start_idx = None\n",
    "    \n",
    "    for i in range(len(non_silent_frames)):\n",
    "        if non_silent_frames[i] and start_idx is None:\n",
    "            # Start of non-silent region\n",
    "            start_idx = librosa.frames_to_samples(i, hop_length=hop_length)\n",
    "        elif not non_silent_frames[i] and start_idx is not None:\n",
    "            # End of non-silent region\n",
    "            end_idx = librosa.frames_to_samples(i, hop_length=hop_length)\n",
    "            non_silent_audio.append(y[start_idx:end_idx])\n",
    "            start_idx = None\n",
    "    \n",
    "    # Handle case where the audio ends with a non-silent region\n",
    "    if start_idx is not None:\n",
    "        non_silent_audio.append(y[start_idx:])\n",
    "    \n",
    "    # Concatenate all non-silent audio parts\n",
    "    stripped_audio = np.concatenate(non_silent_audio)\n",
    "    \n",
    "    # Save the result or return it\n",
    "    return stripped_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "train_y = pd.read_csv(\"original/train_labels.csv\", index_col=0)\n",
    "ss = pd.read_csv(\"original/submission_format.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"raw\"\n",
    "EXPORT_FOLDER = \"raw_preprocessed\"\n",
    "TARGET_SR = 16000   # target sampling rate \n",
    "ORIGINAL_SR = 48000 # original sampling rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.5.1\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host lingchaos-mbp.lan\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mGit commit: 8c868ba70e, branch: stable\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at /Users/lingchm/Library/Caches/DeepFilterNet/DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /Users/lingchm/Library/Caches/DeepFilterNet/DeepFilterNet3/checkpoints/model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cpu\u001b[0m\n",
      "\u001b[32m2024-12-18 12:36:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.12/site-packages/df/checkpoint.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  latest = torch.load(latest, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# load default deepfilternet model\n",
    "model, df_state, _ = init_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 30 minutes to processs all audio files \n",
    "for root, dirs, files in os.walk(INPUT_FOLDER):\n",
    "    for filename in files:\n",
    "         \n",
    "        if \".mp3\" not in filename:\n",
    "            continue \n",
    "    \n",
    "        if \"train_audios_sample\" in root:\n",
    "            continue \n",
    "        \n",
    "        input_path_file = os.path.join(root, filename)\n",
    "        export_path_file = input_path_file.replace(INPUT_FOLDER, EXPORT_FOLDER).replace(\".mp3\", \".wav\")\n",
    "        \n",
    "        # load data\n",
    "        audio, _ = load_audio(input_path_file, sr=ORIGINAL_SR, format=\"mp3\")\n",
    "\n",
    "        # normalization \n",
    "        audio = audio / torch.max(torch.abs(audio))\n",
    "\n",
    "        # noise removal \n",
    "        enhanced_audio = enhance(model, df_state, audio)\n",
    "\n",
    "        # resampling \n",
    "        audio_resampled = librosa.resample(np.asarray(enhanced_audio).flatten(), target_sr=TARGET_SR, orig_sr=ORIGINAL_SR)\n",
    "\n",
    "        # trim beginning and end silence\n",
    "        audio_trimmed, _ = librosa.effects.trim(audio_resampled)\n",
    "\n",
    "        sf.write(export_path_file, audio_trimmed, TARGET_SR)\n",
    "        \n",
    "        print(f\"Processed {input_path_file} into {export_path_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"raw_preprocessed\"\n",
    "EXPORT_FOLDER = \"raw_preprocessed_diarized\"\n",
    "TARGET_SR = 16000\n",
    "ORIGINAL_SR = 48000\n",
    "MIN_SPEAKERS, MAX_SPEAKERS = 1, 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the diarization pipeline\n",
    "pipeline = SpeakerDiarization.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jnpr  Original length 480001, Removed length 8.3%\n",
      "hjiq  Original length 341872, Removed length 5.5%\n",
      "fhsx  Original length 476160, Removed length 0.6%\n",
      "rpou  Original length 478720, Removed length 4.0%\n",
      "koel  Original length 480001, Removed length 79.2%\n",
      "kflj  Original length 472321, Removed length 0.9%\n",
      "vehz  Original length 460896, Removed length 2.5%\n",
      "zjvs  Original length 480000, Removed length 4.9%\n",
      "xgem  Original length 480001, Removed length 1.2%\n",
      "gfxo  Original length 340992, Removed length 4.4%\n",
      "alrq  Original length 480001, Removed length 2.4%\n",
      "mvlw  Original length 480000, Removed length 1.6%\n",
      "dcuv  Original length 275744, Removed length 5.4%\n",
      "ghle  Original length 214016, Removed length 15.2%\n",
      "fgtd  Original length 480001, Removed length 0.1%\n",
      "vmec  Original length 480000, Removed length 0.2%\n",
      "spph  Original length 328176, Removed length 2.4%\n",
      "krpx  Original length 480001, Removed length 0.5%\n",
      "oelh  Original length 462848, Removed length 32.0%\n",
      "jozu  Original length 480001, Removed length 0.1%\n",
      "ylsp  Original length 480000, Removed length 0.2%\n",
      "zefu  Original length 480001, Removed length 4.7%\n",
      "mtbm  Original length 480001, Removed length 1.7%\n",
      "heut  Original length 366080, Removed length 6.4%\n",
      "mxbw  Original length 480000, Removed length 2.5%\n",
      "uauj  Original length 224240, Removed length 4.8%\n",
      "keec  Original length 241376, Removed length 6.8%\n",
      "mjqh  Original length 333616, Removed length 19.1%\n",
      "zkqs  Original length 479488, Removed length 20.6%\n",
      "xmsx  Original length 480001, Removed length 0.7%\n",
      "ypzy  Original length 480001, Removed length 0.1%\n",
      "kegk  Original length 480001, Removed length 0.8%\n",
      "qojc  Original length 242032, Removed length 6.0%\n",
      "upua  Original length 480001, Removed length 0.2%\n",
      "ukas  Original length 480001, Removed length 1.1%\n",
      "yxxo  Original length 191376, Removed length 12.4%\n",
      "axlk  Original length 453601, Removed length 0.2%\n",
      "eqyg  Original length 472321, Removed length 0.4%\n",
      "vppw  Original length 177664, Removed length 1.4%\n",
      "fzco  Original length 480001, Removed length 1.3%\n",
      "ipfe  Original length 480000, Removed length 8.9%\n",
      "eafy  Original length 480000, Removed length 0.2%\n",
      "gwby  Original length 480001, Removed length 21.5%\n",
      "veex  Original length 154032, Removed length 0.8%\n",
      "wzuy  Original length 457728, Removed length 1.4%\n",
      "zsbf  Original length 445440, Removed length 11.1%\n",
      "zjbs  Original length 480000, Removed length 0.8%\n",
      "dlgn  Original length 480001, Removed length 1.7%\n",
      "pzoi  Original length 479488, Removed length 5.6%\n",
      "gyvr  Original length 208080, Removed length 5.1%\n",
      "rnsz  Original length 290544, Removed length 6.4%\n",
      "ptcr  Original length 480000, Removed length 17.3%\n",
      "qnae  Original length 480001, Removed length 0.1%\n",
      "ukar  Original length 469804, Removed length 4.9%\n",
      "zqlx  Original length 294464, Removed length 3.9%\n",
      "eqwm  Original length 191488, Removed length 4.7%\n",
      "dbsd  Original length 480001, Removed length 2.0%\n",
      "cevp  Original length 341824, Removed length 0.1%\n",
      "brgk  Original length 451984, Removed length 0.2%\n",
      "jjpd  Original length 267776, Removed length 6.9%\n",
      "frwu  Original length 247296, Removed length 5.3%\n",
      "hevn  Original length 273280, Removed length 0.8%\n",
      "uavg  Original length 261056, Removed length 0.5%\n",
      "vcga  Original length 479232, Removed length 0.1%\n",
      "vxql  Original length 346272, Removed length 3.9%\n",
      "fkce  Original length 247296, Removed length 1.2%\n",
      "wpmt  Original length 372704, Removed length 4.4%\n",
      "rcts  Original length 480001, Removed length 0.1%\n",
      "zblf  Original length 477696, Removed length 1.4%\n",
      "umyr  Original length 480001, Removed length 0.4%\n",
      "xqvq  Original length 478208, Removed length 0.3%\n",
      "dzoj  Original length 297808, Removed length 3.3%\n",
      "tlly  Original length 480001, Removed length 3.0%\n",
      "zytb  Original length 480001, Removed length 1.6%\n",
      "qcpc  Original length 480001, Removed length 1.8%\n",
      "tuwq  Original length 480000, Removed length 0.2%\n",
      "jmyx  Original length 480001, Removed length 0.9%\n",
      "hneq  Original length 480001, Removed length 1.8%\n",
      "furj  Original length 480001, Removed length 3.4%\n",
      "qfli  Original length 480001, Removed length 0.1%\n",
      "xvhq  Original length 280704, Removed length 11.4%\n",
      "ijoi  Original length 478720, Removed length 1.0%\n",
      "vjuo  Original length 480001, Removed length 0.1%\n",
      "hqhr  Original length 274368, Removed length 15.3%\n",
      "zjvt  Original length 261904, Removed length 1.0%\n",
      "vbxv  Original length 479744, Removed length 1.8%\n",
      "nnty  Original length 480001, Removed length 0.3%\n",
      "zhot  Original length 473600, Removed length 4.7%\n",
      "uudo  Original length 480001, Removed length 0.1%\n",
      "otmd  Original length 480001, Removed length 0.2%\n",
      "vnzs  Original length 480001, Removed length 3.7%\n",
      "ecsy  Original length 480001, Removed length 0.1%\n",
      "bdtr  Original length 480001, Removed length 0.6%\n",
      "pmvs  Original length 402848, Removed length 2.1%\n",
      "wraw_preprocessed  Original length 480001, Removed length 0.1%\n",
      "azlk  Original length 305152, Removed length 26.0%\n",
      "tvri  Original length 480001, Removed length 1.5%\n",
      "uias  Original length 423632, Removed length 1.0%\n",
      "xwva  Original length 413379, Removed length 1.7%\n",
      "gybc  Original length 387072, Removed length 0.5%\n",
      "guab  Original length 480001, Removed length 1.1%\n",
      "uudn  Original length 477184, Removed length 7.2%\n",
      "rrce  Original length 480001, Removed length 15.4%\n",
      "vwbv  Original length 480001, Removed length 3.4%\n",
      "fqef  Original length 480001, Removed length 2.0%\n",
      "zqcc  Original length 381332, Removed length 4.4%\n",
      "mkya  Original length 476929, Removed length 0.9%\n",
      "rrmy  Original length 470016, Removed length 10.6%\n",
      "pxye  Original length 480001, Removed length 4.2%\n",
      "suzi  Original length 480000, Removed length 0.2%\n",
      "fpbu  Original length 480001, Removed length 1.3%\n",
      "anek  Original length 136624, Removed length 17.2%\n",
      "vcsg  Original length 371712, Removed length 0.9%\n",
      "cheh  Original length 471808, Removed length 7.9%\n",
      "vtqu  Original length 129936, Removed length 29.3%\n",
      "ayro  Original length 275664, Removed length 1.7%\n",
      "pwjk  Original length 480000, Removed length 0.2%\n"
     ]
    }
   ],
   "source": [
    "# takes about 700 minutes to process all files \n",
    "\n",
    "df_duration = pd.DataFrame(columns=[\"speaker_duration\", \"pause_duration\", \"pause_ratio\", \"removed_ratio\", \"original_duration\"])\n",
    "\n",
    "for root, dirs, files in os.walk(INPUT_FOLDER):\n",
    "    for filename in files:\n",
    "         \n",
    "        if \"train_audios_sample\" in root:\n",
    "            continue \n",
    "        \n",
    "        uid = filename.replace(\".wav\", \"\")\n",
    "        input_path_file = os.path.join(root, filename)\n",
    "        export_path_file = input_path_file.replace(INPUT_FOLDER, EXPORT_FOLDER)\n",
    "        duration_data = {}\n",
    "        if os.path.exists(export_path_file):\n",
    "            continue \n",
    "        # load data\n",
    "        audio, sr = librosa.load(input_path_file, sr=None)\n",
    "        \n",
    "        # diarization \n",
    "        diarization = pipeline(input_path_file, min_speakers=MIN_SPEAKERS, max_speakers=MAX_SPEAKERS)\n",
    "\n",
    "        # Identify the main speaker \n",
    "        speakers = diarization.labels()\n",
    "        \n",
    "        if len(speakers) == 0:\n",
    "            continue \n",
    "        \n",
    "        multiple_speakers = len(speakers) > 1 \n",
    "        if multiple_speakers:\n",
    "            max_duration, main_speaker = 0, speakers[0]\n",
    "            speaker_duration = {}\n",
    "            for speaker in speakers:\n",
    "                speaker_duration[speaker] = diarization.label_duration(speaker)\n",
    "                # main speaker has max duration \n",
    "                if speaker_duration[speaker] > max_duration: \n",
    "                    main_speaker = speaker\n",
    "                    max_duration = speaker_duration[speaker]\n",
    "            # print(\"Speakers and duration:\", speaker_duration, \"   Main speaker:\", main_speaker)\n",
    "\n",
    "        else:\n",
    "            main_speaker = speakers[0]\n",
    "\n",
    "        # compute pause ratio \n",
    "        speaker_timeline = diarization.label_timeline(main_speaker)\n",
    "        gaps_timeline = diarization.get_timeline().gaps()\n",
    "        pause_duration = gaps_timeline.duration()\n",
    "        speaker_duration = speaker_timeline.duration()\n",
    "        pause_ratio = pause_duration / speaker_duration\n",
    "        duration_data[\"original_duration\"] = diarization.get_timeline().duration()\n",
    "        duration_data[\"pause_duration\"] = pause_duration\n",
    "        duration_data[\"speaker_duration\"] = speaker_duration\n",
    "        duration_data[\"pause_ratio\"] = pause_ratio\n",
    "        \n",
    "        if not multiple_speakers:\n",
    "            sf.write(export_path_file, audio, sr)\n",
    "            continue \n",
    "        \n",
    "        # remove other speaker's segments\n",
    "        keep_timeline = speaker_timeline.union(gaps_timeline)\n",
    "        speaker_audio = []\n",
    "        for segment in keep_timeline:\n",
    "            start_sample = int(segment.start * sr)\n",
    "            end_sample = int(segment.end * sr)\n",
    "            speaker_audio.append(audio[start_sample:end_sample])\n",
    "        speaker_audio = np.concatenate(speaker_audio)\n",
    "        removed_ratio = 1 - speaker_audio.shape[0] / audio.shape[0]\n",
    "        print(f\"{uid}  Original length {audio.shape[0]}, Removed length {removed_ratio * 100:.1f}%\")\n",
    "        duration_data[\"removed_ratio\"] = speaker_duration\n",
    "        \n",
    "        df_duration.loc[uid] = duration_data\n",
    "        \n",
    "        sf.write(export_path_file, speaker_audio, sr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration.to_csv(\"preprocessed/diariation_details.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x31f80fe90>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test diarization\n",
    "uid = \"lgid\"\n",
    "input_path_file = os.path.join(INPUT_FOLDER, f\"test_audios/{uid}.wav\")\n",
    "audio, sr = librosa.load(input_path_file, sr=None)\n",
    "diarization = pipeline(input_path_file, min_speakers=MIN_SPEAKERS, max_speakers=MAX_SPEAKERS)\n",
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers and duration: {'SPEAKER_00': 6.429375000000004, 'SPEAKER_01': 12.825}    Main speaker: SPEAKER_01\n",
      "Original length 480001, Main speaker length 380956 (20.6%)\n"
     ]
    }
   ],
   "source": [
    "# Identify the main speaker \n",
    "speakers = diarization.labels()\n",
    "multiple_speakers = len(speakers) > 1 \n",
    "if multiple_speakers:\n",
    "    max_duration, main_speaker = 0, speakers[0]\n",
    "    speaker_duration = {}\n",
    "    for speaker in speakers:\n",
    "        speaker_duration[speaker] = diarization.label_duration(speaker)\n",
    "        # main speaker has max duration \n",
    "        if speaker_duration[speaker] > max_duration: \n",
    "            main_speaker = speaker\n",
    "            max_duration = speaker_duration[speaker]\n",
    "    print(\"Speakers and duration:\", speaker_duration, \"   Main speaker:\", main_speaker)\n",
    "\n",
    "else:\n",
    "    main_speaker = speakers[0]\n",
    "\n",
    "# compute pause ratio \n",
    "speaker_timeline = diarization.label_timeline(main_speaker)\n",
    "gaps_timeline = diarization.get_timeline().gaps()\n",
    "pause_duration = gaps_timeline.duration()\n",
    "speaker_duration = speaker_timeline.duration()\n",
    "pause_ratio = pause_duration / speaker_duration\n",
    "\n",
    "if multiple_speakers:\n",
    "    # remove other speaker's segments\n",
    "    keep_timeline = speaker_timeline.union(gaps_timeline)\n",
    "    speaker_audio = []\n",
    "    for segment in keep_timeline:\n",
    "        start_sample = int(segment.start * sr)\n",
    "        end_sample = int(segment.end * sr)\n",
    "        speaker_audio.append(audio[start_sample:end_sample])\n",
    "    speaker_audio = np.concatenate(speaker_audio)\n",
    "    removed_ratio = 1 - speaker_audio.shape[0] / audio.shape[0]\n",
    "    print(f\"Original length {audio.shape[0]}, Main speaker length {speaker_audio.shape[0]} ({removed_ratio * 100:.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
