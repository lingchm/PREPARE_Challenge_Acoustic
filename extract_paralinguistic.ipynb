{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Paralinguistic Features from Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paralinguistic features of audio files refer to the aspects of speech that convey information about the speaker’s emotional state, personality, social identity, and other non-verbal cues that accompany spoken language. These features are distinct from the linguistic content (the words themselves) and focus more on how something is said rather than what is said. Paralinguistic features can be used to understand nuances like tone, emotion, and intent in speech, which are important for applications in emotion recognition, sentiment analysis, and speaker profiling.\n",
    "\n",
    "\n",
    "This notebook extracts three types of paralinguistic features: \n",
    "* MFCC\n",
    "* eGEMAPS\n",
    "* ComPAre\n",
    "\n",
    "The code assumes that the raw audio signals have been preprocessed. For preprocessing, refer to the other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensmile\n",
    "import audiofile\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import os \n",
    "import soundfile as sf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "train_y = pd.read_csv(\"original/train_labels.csv\", index_col=0)\n",
    "ss = pd.read_csv(\"original/submission_format.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC (Mel-Frequency Cepstral Coefficients) are a widely used feature extraction method for speech and audio processing. They represent the short-term power spectrum of an audio signal and are used extensively in speech recognition, speaker identification, and other audio processing tasks.\n",
    "\n",
    "MFCCs are derived from the Fourier transform of an audio signal, which provides a frequency-domain representation of the signal. However, MFCCs are designed to more closely resemble the way humans perceive sound, which makes them effective for audio and speech-related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"raw_preprocessed\"\n",
    "FILE_FORMAT = \"wav\" # mp3\n",
    "n_mfcc = 20  # Number of MFCC features to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1646/1646 [00:29<00:00, 55.95it/s]\n",
      "100%|██████████| 412/412 [00:07<00:00, 58.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_mfcc = pd.DataFrame(index=train_y.index, columns=[f\"mfcc_{i+1}\" for i in range(n_mfcc)])\n",
    "\n",
    "for uid in tqdm(train_mfcc.index):\n",
    "    if \"wraw\" in uid: continue \n",
    "    signal, sr = librosa.load(f\"{INPUT_FOLDER}/train_audios/{uid}.{FILE_FORMAT}\", sr=None)\n",
    "    signal = signal / np.max(np.abs(signal))  \n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Take the mean of MFCCs across frames (to get a single feature vector per audio file)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)  # This will give you a 1D array with n_mfcc features\n",
    "    train_mfcc.loc[uid] = mfcc_mean\n",
    "\n",
    "test_mfcc = pd.DataFrame(index=ss.index, columns=[f\"mfcc_{i+1}\" for i in range(n_mfcc)])\n",
    "for uid in tqdm(test_mfcc.index):\n",
    "    signal, sr = librosa.load(f\"{INPUT_FOLDER}/test_audios/{uid}.{FILE_FORMAT}\", sr=None)\n",
    "    signal = signal / np.max(np.abs(signal))  # Normalizing the audio\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Take the mean of MFCCs across frames (to get a single feature vector per audio file)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)  # This will give you a 1D array with n_mfcc feature\n",
    "    test_mfcc.loc[uid] = mfcc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mfcc.to_csv(\"paralinguistic/train_mfcc_features_v2.csv\", index=True)\n",
    "test_mfcc.to_csv(\"paralinguistic/test_mfcc_features_v2.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eGEMAPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eGeMAPSv02 is a set of audio features primarily used for voice and speech analysis. These features include:\n",
    "\n",
    "* Low-level descriptors (LLDs) such as pitch, energy, formants, and spectral features.\n",
    "* Statistical functionals to summarize these LLDs over time.\n",
    "\n",
    "These features are typically extracted from the entire audio signal, including both speech and silence segments. In general, silence regions do not contribute meaningful information for these features. However, silence can still influence certain aspects of the signal, like energy and formant frequencies, especially if the silence is of significant duration or if there's background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"raw_preprocessed\"\n",
    "FILE_FORMAT = \"wav\" # mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "smile1 = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")\n",
    "print(len(smile1.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_silence(y, sr, silence_threshold=0.02, frame_length=2048, hop_length=512, output_path=None):\n",
    "    \"\"\"\n",
    "    Strip silence from an audio file and save the resulting audio without silence.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path (str): Path to the input audio file.\n",
    "        silence_threshold (float): Energy threshold below which the region is considered silent.\n",
    "        frame_length (int): Length of each frame to analyze in samples.\n",
    "        hop_length (int): The number of samples to shift for each frame.\n",
    "        output_path (str, optional): Path to save the output audio without silence. If None, it will return the numpy array.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Audio signal with silence removed (if no output_path is provided).\n",
    "    \"\"\"\n",
    "    # Compute the energy (Root Mean Square) of the audio signal\n",
    "    energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    \n",
    "    # Identify non-silent regions where energy is above the threshold\n",
    "    non_silent_frames = energy > silence_threshold\n",
    "    \n",
    "    # Get the time intervals corresponding to non-silent frames\n",
    "    times = librosa.frames_to_time(np.arange(len(energy)), sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Extract the non-silent portions of the audio\n",
    "    non_silent_audio = []\n",
    "    start_idx = None\n",
    "    \n",
    "    for i in range(len(non_silent_frames)):\n",
    "        if non_silent_frames[i] and start_idx is None:\n",
    "            # Start of non-silent region\n",
    "            start_idx = librosa.frames_to_samples(i, hop_length=hop_length)\n",
    "        elif not non_silent_frames[i] and start_idx is not None:\n",
    "            # End of non-silent region\n",
    "            end_idx = librosa.frames_to_samples(i, hop_length=hop_length)\n",
    "            non_silent_audio.append(y[start_idx:end_idx])\n",
    "            start_idx = None\n",
    "    \n",
    "    # Handle case where the audio ends with a non-silent region\n",
    "    if start_idx is not None:\n",
    "        non_silent_audio.append(y[start_idx:])\n",
    "    \n",
    "    # Concatenate all non-silent audio parts\n",
    "    stripped_audio = np.concatenate(non_silent_audio)\n",
    "    \n",
    "    # Save the result or return it\n",
    "    return stripped_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1646/1646 [00:29<00:00, 55.95it/s]\n",
      "100%|██████████| 412/412 [00:07<00:00, 58.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_gemaps = pd.DataFrame(index=train_y.index, columns=smile1.feature_names)\n",
    "for uid in tqdm(train_gemaps.index):\n",
    "    try:\n",
    "        signal, sr = librosa.load(f\"{INPUT_FOLDER}/train_audios/{uid}.{FILE_FORMAT}\", sr=None)\n",
    "        # signal = signal / np.max(np.abs(signal))\n",
    "        # signal_nosilence = strip_silence(signal, sr)\n",
    "        output = smile1.process_signal(signal, sampling_rate=sr)\n",
    "        train_gemaps.loc[uid] = np.asarray(output[smile1.feature_names])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "test_gemaps = pd.DataFrame(index=ss.index, columns=smile1.feature_names)\n",
    "for uid in tqdm(test_gemaps.index):\n",
    "    signal, sr = librosa.load(f\"{INPUT_FOLDER}/test_audios/{uid}.{FILE_FORMAT}\", sr=None)\n",
    "    # signal = signal / np.max(np.abs(signal))\n",
    "    # signal_nosilence = strip_silence(signal, sr)\n",
    "    output = smile1.process_signal(signal, sampling_rate=sr)\n",
    "    test_gemaps.loc[uid] = np.asarray(output[smile1.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gemaps.to_csv(\"paralinguistic/train_gemaps_features_v2.csv\", index=True)\n",
    "test_gemaps.to_csv(\"paralinguistic/test_gemaps_features_v2.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComPAre\n",
    "\n",
    "ComPAre (Common Phonetic Audio Representation) is a set of audio features designed for capturing phonetic and prosodic information from speech, typically used in speech recognition and other speech-related tasks. These features are derived from the raw audio signal to represent the speech in a compact and more informative way, highlighting characteristics such as pitch, intensity, formants, and speech rhythms.\n",
    "\n",
    "ComPAre features are particularly useful when analyzing prosodic features of speech, including emotional tone, stress, and intonation patterns, as well as for tasks like speech-to-text and speaker identification.\n",
    "\n",
    "ComPAre features are typically high-level speech features extracted from the raw audio signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"raw_preprocessed\"\n",
    "FILE_FORMAT = \"wav\" # mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "smile2 = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors_Deltas\n",
    ")\n",
    "print(len(smile2.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1646/1646 [00:29<00:00, 55.95it/s]\n",
      "100%|██████████| 412/412 [00:07<00:00, 58.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_compare = pd.DataFrame(index=train_y.index, columns=smile2.feature_names)\n",
    "for uid in tqdm(train_compare.index):\n",
    "    try:\n",
    "        signal, sr = librosa.load(f\"{INPUT_FOLDER}/train_audios/{uid}.{FILE_FORMAT}\", sr=None)\n",
    "        # signal = signal / np.max(np.abs(signal))\n",
    "        # signal_nosilence = strip_silence(signal, sr)\n",
    "        output = smile2.process_signal(signal, sampling_rate=sr)\n",
    "        train_compare.loc[uid] = np.asarray(output.mean(axis=0)[smile2.feature_names])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "test_compare = pd.DataFrame(index=ss.index, columns=smile2.feature_names)\n",
    "for uid in tqdm(test_gemaps.index):\n",
    "    signal, sr = librosa.load(f\"{INPUT_FOLDER}/test_audios/{uid}.{FILE_FORMAT}\", sr=None)\n",
    "    # signal = signal / np.max(np.abs(signal))\n",
    "    # signal = strip_silence(signal, sr)\n",
    "    output = smile2.process_signal(signal, sampling_rate=sr)\n",
    "    test_compare.loc[uid] = np.asarray(output.mean(axis=0)[smile2.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_compare.to_csv(\"paralinguistic/train_compare_features_v2.csv\", index=True)\n",
    "test_compare.to_csv(\"paralinguistic/test_compare_features_v2.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
